---
title: "Solving the Game of SET: Part II"
excerpt: "A slight improvement."
last_modified_at: 2020-04-14
permalink: /set2/
header:
  teaser: /assets/images/set/set_thumbnail2.png
tags: 
  - Projects
  - Computer Vision
---

#### Recap - Part I
Be sure to read through <a href="/set/" target="_blank">Part I</a> for some background on my initial solution.
We left off with a really amazing program that can solve a game of set on a static image.

Unfortunately, when I ran the solution on a live video, you can see a few flaws in my solution:  

<iframe width="560" height="315" src="https://www.youtube.com/embed/3HQy8-wSRLc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

#### Issues to Address

Here's what I'll try and fix in Part II:
1. Fix solution jitters in live stream
   * Applying the static image solution to a live sream naively results in a ton of jumping around because the solution is not sorted based on location. 

2. More robust fill detection
   * Use the corner of the card as a reference value to determine what zero-fill looks like.

3. More robust color solution
   * The color solution relies on the camera and on the lighting. This game uses color but the actual color doesn't matter - it only matters that colors are *grouped* appropriately. We can classify the colors holistically by looking at the entire scene instead of on a card-by-card basis.

4. Apply solution when change is detected
  * This one might not be important, but it more accurately reflects on humans perceive the game and recognize change. We'll try to only apply our solution when the board has changed, not on every single frame. This should also help eliminate solution jitters.


#### 1. Fixing Card Locations
The first step in fixing these jitters are to sort the card array by *(x,y)* location. Currently the array is left un-sorted (or rather it's sorted by size of the card) which leaves the final sort pretty random. This doesn't affect the solution technically, because the game isn't dependent on card location. On still images, it doesn't matter but it does affect how the solution gets displayed in a live-feed. Videos are always trickier than still images...

First, we fix the card location like so:
```python
def generate_cards(self):
    info = []
    for card in self.card_array:
        self.card_parameters.append([card.number, card.shape, card.fill, card.color])
        info.append([card.x, card.y, card.number, card.shape, card.fill, card.color])
    info = np.array(info)
    iidx = np.argsort(info[:, 0] + 2*info[:, 1])
    self.card_info = info[idx]
```
This method is pretty arbitrary but it works really well and is very simple.

<figure class = "align-right">
    <a href="/assets/images/set/hash_or_empty.jpg"><img src="/assets/images/set/hash_or_empty.jpg"></a>
    <figcaption>We compare the two patches outlined in black.</figcaption>
</figure>

#### 2. More robust fill detection
The next issue I found was my method for determining symbol fill wasn't as great or robust as I thought. Some of the jumping around in the solution is because my analysis of the cards is jumping around. My original method for fill analysis was to run an edge detector on the fill and if we found edges then we knew it was hash filled.
Unfortunately, this doesn't work well or robustly with low-quality webcam images because the resolution isn't fine enough to distinguish between the hash lines. I found that my original method of comparing the euclidean distance between a patch at the center of the symbol and a patch in the corner of the card worked much better.
As you can tell, the green is particularily tricky because the camera gives things quite a bit of green tinge already.

The final things I did to create a more robust solution is specific to video feeds.
We can eliminate a lot of noise in our feed by averaging frames in our feed using cv2.accumlateWeighted().
```python
cv2.accumulateWeighted(next_img, result, .4)
result = cv2.convertScaleAbs(result)
```
I also check for motion in the frame by subtracting the images in subsequent frames and thresholding the result. If the frames are identical, the result will be a purely black frame. If there are any differences, we will see white pixels. We can check for motion easily just by adding up our white pixels and setting a threshold value. In addition, we'll maintain a buffer so we need to see motion for *n* frames in a row. We can use the *pop* method of *python* lists to make this easy. 

```python
diff = cv2.subtract(result, old_frame)
thresh_diff = threshold_img(bw_filter(diff))
thresh_buffer.append(thresh_diff.sum())

if not len(thresh_buffer) < n:
    thresh_buffer.pop(0)

if not np.all(np.array(thresh_buffer)) > motion_threshold:
    """
    If no motion is detected, process scene here
    """ 
else:
    """
    If motion is detected, do nothing
    """
    output = image
```

Finally we can visualize the results:
<iframe width="560" height="315" src="https://www.youtube.com/embed/7TH_z8l6PTc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Here's an entire deck being played through:



